# -*- coding: utf-8 -*-
"""Llama 3 - Hugging Face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQJfzR35sO3VsU0Kb96t0ePjCIhaNjTN

**Installing the Dependencies**
"""

!pip install -r requirements.txt

import json
import requests

"""**HF account Configuration**"""

# Load the model configuration
config_data = json.load(open("config.json"))
model_name_or_path = config_data["_name_or_path"]

# API token
HF_TOKEN = "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"

API_URL = f"https://api-inference.huggingface.co/models/{model_name_or_path}"
headers = {
    "Authorization": f"Bearer {HF_TOKEN}"
}

def query_huggingface_api(prompt):
    response = requests.post(API_URL, headers=headers, json={"inputs": prompt})
    return response.json()

def get_response(prompt):
    response = query_huggingface_api(prompt)
    gen_text = response[0]["generated_text"]
    return gen_text

prompt = "What is Machine Learning?"

llama3_response = get_response(prompt)

print(llama3_response)

print(llama3_response[len(prompt):])
