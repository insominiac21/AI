# -*- coding: utf-8 -*-
"""Llama 3 - Hugging Face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQJfzR35sO3VsU0Kb96t0ePjCIhaNjTN

**Installing the Dependencies**
"""

!pip install -r requirements.txt

import json
import requests

"""**HF account Configuration**"""

config_data = json.load(open("config.json"))
HF_TOKEN = config_data["HF_TOKEN"]

API_URL = "https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B"
headers = {
    "Authorization": f"Bearer {HF_TOKEN}"
}

def query_huggingface_api(prompt):
    response = requests.post(API_URL, headers=headers, json={"inputs": prompt})
    return response.json()

def get_response(prompt):
    response = query_huggingface_api(prompt)
    gen_text = response[0]["generated_text"]
    return gen_text

prompt = "What is Machine Learning?"

llama3_response = get_response(prompt)

print(llama3_response)

print(llama3_response[len(prompt):])
